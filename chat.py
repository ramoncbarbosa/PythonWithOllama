import os
import json
import uuid
import requests
import time

# -------- CONFIGS OLLAMA --------
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "llama3"
TIMEOUT = 60
MAX_TOKENS = 800
TEMPERATURE = 0.6

categories = [
    "vari√°veis",
    "listas",
    "condicionais",
    "fun√ß√µes",
    "compara√ß√µes",
    "textos_e_caracteres",
    "n√∫meros",
    "operadores_matem√°ticos",
    "operadores_l√≥gicos",
]

subcategories = {
    "vari√°veis": ["o_que_s√£o_vari√°veis", "tipos_existentes_de_vari√°veis", "vari√°veis_mut√°veis", "escopo_local_da_vari√°vel", "escopo_global_da_vari√°vel"],
    "listas": ["o_que_s√£o_listas", "adicionar_elemento_na_lista", "remover_elemento_da_lista", "ordenar_lista", "acessar_elemento_da_lista"],
    "condicionais": ["o_que_s√£o_condicionais", "condicional_simples", "condicional_composta", "condicional_aninhada"],
    "fun√ß√µes": ["o_que_s√£o_fun√ß√µes", "como_usar_fun√ß√µes", "tipos_de_fun√ß√µes"],
    "compara√ß√µes": ["compara√ß√£o_entre_valores", "maior_ou_menor", "compara√ß√£o_de_conte√∫do", "impacto_da_compara√ß√£o"],
    "textos_e_caracteres": ["o_que_s√£o_textos", "opera√ß√µes_de_concatenacao_de_textos", "o_que_s√£o_caracteres", "como_converter_caracteres"],
    "n√∫meros": ["o_que_s√£o_n√∫meros_inteiros", "o_que_s√£o_n√∫meros_decimais"],
    "operadores_matem√°ticos": ["operadores_matem√°ticos_b√°sicos"],
    "operadores_l√≥gicos": ["o_que_s√£o_operadores_l√≥gicos"],
}

def generate_prompt(subcategoria):
    return f"Considere o t√≥pico '{subcategoria}'. Gere 5 perguntas de m√∫ltipla escolha sobre esse t√≥pico. Cada pergunta deve conter exatamente 5 op√ß√µes de resposta, variadas. Inclua um campo answer_keys como array, onde cada item √© a letra da resposta correta ('A', 'B', 'C', 'D' ou 'E'). Responda apenas com o JSON bruto, sem explica√ß√£o e sem usar markdown. N√£o escreva nada al√©m do JSON."

def call_ollama(prompt: str) -> str:
    payload = {
        "model": OLLAMA_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "options": {"temperature": TEMPERATURE, "num_predict": MAX_TOKENS},
        "stream": False
    }

    try:
        print(f"üì° OLLAMA-{OLLAMA_MODEL}")
        resp = requests.post(OLLAMA_URL, json=payload, timeout=TIMEOUT)
        resp.raise_for_status()
        response_json = resp.json()
        return response_json["message"]["content"]

    except requests.exceptions.RequestException as e:
        print(f"üåê Erro OLLAMA: {e}")
        return ""

def create_quiz_json(categoria, subcategoria):
    response = call_ollama(generate_prompt(subcategoria))
    quiz = json.loads(response)

    return {
        "group_id": str(uuid.uuid4()),
        "category": categoria,
        "subject": subcategoria,
        "prompt": generate_prompt(subcategoria),
        "source": "ollama",
        "questions": quiz["questions"],
        "answer_keys": quiz["answer_keys"]
    }

all_quizzes = []

for categoria in categories:
    for subcategoria in subcategories[categoria]:
        quiz_gerado = create_quiz_json(categoria, subcategoria)
        all_quizzes.append(quiz_gerado)

output_filename = "todos_quizzes_ollama.json"

with open(output_filename, 'w', encoding='utf-8') as f:
    json.dump(all_quizzes, f, ensure_ascii=False, indent=4)

print(f"Todos quizzes salvos em: {output_filename}")
